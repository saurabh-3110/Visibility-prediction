{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1b07eb6-fd1c-473f-83e8-5781b29b1cac"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import csv,os,re,sys,codecs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib,  statistics\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.feature_selection import SelectKBest,chi2\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfrSvsL366pP"
      },
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSoWgttX7Olp"
      },
      "source": [
        "***Importing and sorting Values:*** The dataset was imported and sorted according to time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ba506dc-bd5e-4629-b85d-e1aa6bec2ceb"
      },
      "outputs": [],
      "source": [
        "######## Impporting both training and test data and comcantating ########\n",
        "path_x=\"given_data/spatiotemporal_trn_data.csv\"\n",
        "path_target=\"given_data/spatiotemporal_trn_targets.csv\"\n",
        "weather=pd.read_csv(path_x,dtype=object)\n",
        "y=pd.read_csv(path_target,header=None)\n",
        "y.columns = ['Column1', 'Value']\n",
        "weather['visibility']=y['Value']\n",
        "\n",
        "\n",
        "####### Sorting values ##############\n",
        "weather['DATE'] = pd.to_datetime(weather['DATE'])\n",
        "weather= weather.sort_values(by=['NAME','DATE'])\n",
        "weather.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Percentage of Nan values in each column:*** Only features with less than 2% NaN values are selected with some exception"
      ],
      "metadata": {
        "id": "MvhuIabiD5DO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTo0PaqpDxpX"
      },
      "outputs": [],
      "source": [
        "############ Missing Values ##########\n",
        "\n",
        "#pd.set_option('display.max_rows', None)     # comment this out to see all rows\n",
        "nan_percentage = (weather.isna().mean() * 100).round(2)\n",
        "\n",
        "print(\"Percentage of NaN values for each column:\")\n",
        "print(nan_percentage)\n",
        "pd.reset_option('display.max_rows')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi-uMl8b7k8K"
      },
      "source": [
        "**Extracting relevant features(Hour and Month):** Hour gives us information about the time of the day that is essentially the amount of sunlight. Month gives us the idea about the ongoing season at the time of recording\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_qKFVZiyRnz"
      },
      "outputs": [],
      "source": [
        "weather['hour'] = weather['DATE'].dt.hour          #### Hour of the date in 24 hr format ####\n",
        "weather['month'] = weather['DATE'].dt.month        #### Month of the year in number ########\n",
        "weather=weather[[\"hour\",\"month\",\"ELEVATION\",\"REPORT_TYPE\",\"HourlyDewPointTemperature\",\"HourlyDryBulbTemperature\",\"HourlyPresentWeatherType\",\"HourlyRelativeHumidity\",\"HourlySeaLevelPressure\",\"HourlyWindDirection\",\"HourlyWindSpeed\",\"visibility\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPE-SgVP8TmP"
      },
      "source": [
        "***Data Cleaning (Removing Character Anamoly):***Numeric features excluding target variable had 's' in the values whereas target variable had 'V'in their values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sg2Aw6SAUTh4"
      },
      "outputs": [],
      "source": [
        "######### Removing character anamoly ########\n",
        "\n",
        "##### For target variable\n",
        "\n",
        "pattern = r'^(\\d+)V$'   ## Identifying the anomaly\n",
        "weather['visibility'] = weather['visibility'].apply(lambda x: re.sub(pattern, r'\\1', str(x)) if isinstance(x, str) else x)\n",
        "# Convert the 'visibility' to integers using pd.to_numeric, and coerce to handle NaN values\n",
        "weather['visibility'] = pd.to_numeric(weather['visibility'], errors='coerce')\n",
        "\n",
        "##### For features\n",
        "pattern = r'^(\\d+)s$'\n",
        "# Loop through the columns you want to transform\n",
        "columns_to_transform = ['HourlyDewPointTemperature', 'HourlyDryBulbTemperature','HourlyRelativeHumidity','HourlySeaLevelPressure','HourlyWindDirection','HourlyWindSpeed']\n",
        "\n",
        "for column in columns_to_transform:\n",
        "    weather[column] = weather[column].apply(lambda x: re.sub(pattern, r'\\1', str(x)) if isinstance(x, str) else x)\n",
        "    # Convert the column to integers using pd.to_numeric, and coerce to handle NaN values\n",
        "    weather[column] = pd.to_numeric(weather[column], errors='coerce')\n",
        "\n",
        "\n",
        "\n",
        "weather['HourlyWindDirection'] = weather['HourlyWindDirection'].replace('VRB', 0)\n",
        "weather[['ELEVATION','HourlyDewPointTemperature', 'HourlyDryBulbTemperature','HourlyRelativeHumidity','HourlySeaLevelPressure','HourlyWindDirection','HourlyWindSpeed','visibility']] = weather[['ELEVATION','HourlyDewPointTemperature', 'HourlyDryBulbTemperature','HourlyRelativeHumidity','HourlySeaLevelPressure','HourlyWindDirection','HourlyWindSpeed','visibility']].astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6E-KmeUDxpZ"
      },
      "source": [
        "**Target Variable:** Understanding the target variable and its distribution,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GJNOXRPDxpZ"
      },
      "outputs": [],
      "source": [
        "min=weather['visibility'].min()\n",
        "max= weather['visibility'].max()\n",
        "print(f\"Minimum visibility:- {min},Maximum visibility:- {max}\" )\n",
        "\n",
        "## Plotting distribution of target variable\n",
        "bin_edges = np.arange(0, 101, 5)\n",
        "weather['visibility'].plot.hist(bins=bin_edges)\n",
        "plt.xlabel(\"Visibility\")\n",
        "plt.title(\"Distribution of Target Variable\")\n",
        "plt.xticks(bin_edges)\n",
        "plt.savefig('visibility.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBIRmKM4DxpZ"
      },
      "source": [
        "***Rows containing NaN values :*** Getting a idea on missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4pNXXleDxpZ"
      },
      "outputs": [],
      "source": [
        "nan_rows = weather.isnull().any(axis=1)\n",
        "nan_count = nan_rows.sum()\n",
        "print(\"Total number of rows with NaN values:\", nan_count)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9eRnFG5Dxpa"
      },
      "source": [
        "***Knowing Minimum and Maximum values of features:*** To decide weather to scale the varibale or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRudDBuTDxpa"
      },
      "outputs": [],
      "source": [
        "weather_2=weather[[\"hour\",\"month\",\"ELEVATION\",\"HourlyDewPointTemperature\",\"HourlyDryBulbTemperature\",\"HourlyRelativeHumidity\",\"HourlySeaLevelPressure\",\"HourlyWindSpeed\",\"visibility\"]]\n",
        "\n",
        "min_values = weather_2.min()\n",
        "max_values = weather_2.max()\n",
        "# Display the results\n",
        "print(\"\\nMinimum values for each column:\")\n",
        "print(min_values)\n",
        "\n",
        "print(\"\\nMaximum values for each column:\")\n",
        "print(max_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZNi9NRU8-AV"
      },
      "source": [
        "***Column transforming:*** Non-Numeric relevant features are One-Hot Encoded to be used in model training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94abd69d-8a8d-4e1a-b4f3-20224dbbc58a"
      },
      "outputs": [],
      "source": [
        "############## One Hot Encoding ###########\n",
        "\n",
        "transformer = ColumnTransformer(transformers=[\n",
        "    ('tnf1',OneHotEncoder(sparse=False,drop='first',handle_unknown='infrequent_if_exist',min_frequency=2000),['REPORT_TYPE']),\n",
        "    ('tnf2',OneHotEncoder(sparse=False,drop='first',handle_unknown='infrequent_if_exist',min_frequency=1000),['HourlyPresentWeatherType'])\n",
        "],remainder='passthrough')\n",
        "\n",
        "\n",
        "weather_1=transformer.fit_transform(weather)\n",
        "transformed_df = pd.DataFrame(weather_1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Imputing Values :*** After evaluating performance of different imputing techniques 'Median' was fimalised"
      ],
      "metadata": {
        "id": "LKeG7S8NI7lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Imputing Values #################\n",
        "\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "transformed_df=imputer.fit_transform(transformed_df)\n",
        "\n",
        "transformed_df = pd.DataFrame(transformed_df)\n",
        "transformed_df.to_csv('table.csv', index=False)"
      ],
      "metadata": {
        "id": "J4Qz5TB9Ijnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training"
      ],
      "metadata": {
        "id": "atU3wj0bJxpK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Hyper-parameter tuning and Evaluation metrics***\n",
        "\n"
      ],
      "metadata": {
        "id": "RZSAiNDNJbHw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOKmVekxDxpa"
      },
      "outputs": [],
      "source": [
        "##################################################################\n",
        "########## GRID SEARCH WITHOUT SCALING ###########################\n",
        "##################################################################\n",
        "\n",
        "\n",
        "class regression():\n",
        "     def __init__(self,path='table.csv',rgr_opt='lr',no_of_selected_features=None):\n",
        "        self.path = path\n",
        "        self.rgr_opt=rgr_opt\n",
        "        self.no_of_selected_features=no_of_selected_features\n",
        "        if self.no_of_selected_features!=None:\n",
        "            self.no_of_selected_features=int(self.no_of_selected_features)\n",
        "\n",
        "# Selection of regression techniques\n",
        "     def regression_pipeline(self):\n",
        "    # AdaBoost\n",
        "        if self.rgr_opt=='ab':\n",
        "            print('\\n\\t### AdaBoost Regression ### \\n')\n",
        "            be1 = DecisionTreeRegressor(max_depth=10,ccp_alpha=0.02,random_state=0)\n",
        "            be2 = Ridge(alpha=1.0,solver='lbfgs',positive=True)\n",
        "\n",
        "            rgr = AdaBoostRegressor(n_estimators=100)\n",
        "            rgr_parameters = {\n",
        "            'rgr__estimator':(be1,be3,be3),\n",
        "            'rgr__random_state':(0,10),\n",
        "            }\n",
        "    # Decision Tree\n",
        "        elif self.rgr_opt=='dt':\n",
        "            print('\\n\\t### Decision Tree ### \\n')\n",
        "            rgr = DecisionTreeRegressor(random_state=40)\n",
        "            rgr_parameters = {\n",
        "            'rgr__criterion':('squared_error','friedman_ms','absolute_error', 'poisson'),\n",
        "            'rgr__max_depth':(30,None),\n",
        "            'rgr__ccp_alpha':(0.009,0.00),\n",
        "            }\n",
        "    # Ridge Regression\n",
        "        elif self.rgr_opt=='rg':\n",
        "            print('\\n\\t### Ridge Regression ### \\n')\n",
        "            rgr = Ridge(alpha=1.0,positive=True)\n",
        "            rgr_parameters = {\n",
        "            'rgr__solver':('auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs'),\n",
        "            }\n",
        "    # Linear Regression\n",
        "        elif self.rgr_opt=='lr':\n",
        "            print('\\n\\t### Linear Regression ### \\n')\n",
        "            rgr = LinearRegression()\n",
        "            rgr_parameters = {\n",
        "            'rgr__positive':(True,False),\n",
        "            }\n",
        "    # Random Forest\n",
        "        elif self.rgr_opt=='rf':\n",
        "            print('\\n\\t ### Random Forest ### \\n')\n",
        "            rgr = RandomForestRegressor(max_features=None)\n",
        "            rgr_parameters = {\n",
        "            #'rgr__criterion':('squared_error','friedman_mse','poisson'),\n",
        "            'rgr__n_estimators':(100,130,150),\n",
        "            'rgr__max_depth':(50,70,None),\n",
        "            'rgr__max_features':(1,5,10),\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            print('Select a valid classifier \\n')\n",
        "            sys.exit(0)\n",
        "        return rgr,rgr_parameters\n",
        "\n",
        "# Load the data\n",
        "     def get_data(self):\n",
        "    # Load the file using CSV Reader\n",
        "        # fl=open(self.path+'winequality_white.csv',\"r\")\n",
        "        # reader = list(csv.reader(fl,delimiter='\\n'))\n",
        "        # fl.close()\n",
        "        # data=[]; labels=[];\n",
        "        # for item in reader[1:]:\n",
        "        #     item=''.join(item).split(';')\n",
        "        #     labels.append(item[-1])\n",
        "        #     data.append(item[:-1])\n",
        "        # # labels=[int(''.join(item)) for item in labels]\n",
        "        # data=np.asarray(data)\n",
        "\n",
        "    # Load the file using Pandas\n",
        "        reader=pd.read_csv('table.csv')\n",
        "\n",
        "    # Select all rows except the ones belong to particular class'\n",
        "        # mask = reader['class'] == 9\n",
        "        # reader = reader[~mask]\n",
        "\n",
        "        data=reader.iloc[:, :-1]\n",
        "        labels=reader.iloc[:,-1]\n",
        "\n",
        "        # Training and Test Split\n",
        "        training_data, validation_data, training_cat, validation_cat = train_test_split(data, labels,\n",
        "                                               test_size=0.3, random_state=42)\n",
        "\n",
        "        return training_data, validation_data, training_cat, validation_cat\n",
        "\n",
        "# Regression using the Gold Statndard after creating it from the raw text\n",
        "     def regression(self):\n",
        "   # Get the data\n",
        "        training_data, validation_data, training_cat, validation_cat=self.get_data()\n",
        "\n",
        "        rgr,rgr_parameters=self.regression_pipeline()\n",
        "        pipeline = Pipeline([('rgr', rgr),])\n",
        "        grid = GridSearchCV(pipeline,rgr_parameters,scoring='f1_macro',cv=10)\n",
        "        grid.fit(training_data,training_cat)\n",
        "        rgr= grid.best_estimator_\n",
        "        print('\\n\\n The best set of parameters of the pipiline are: ')\n",
        "        print(rgr)\n",
        "        joblib.dump(rgr, self.path+'training_model.joblib')\n",
        "        predicted=rgr.predict(validation_data)\n",
        "\n",
        "\n",
        "    # Regression report\n",
        "        mse=mean_squared_error(validation_cat,predicted,squared=True)\n",
        "        print ('\\n MSE:\\t'+str(mse))\n",
        "        rmse=mean_squared_error(validation_cat,predicted,squared=False)\n",
        "        print ('\\n RMSE:\\t'+str(rmse))\n",
        "        r2=r2_score(validation_cat,predicted,multioutput='variance_weighted')\n",
        "        print ('\\n R2-Score:\\t'+str(r2))\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "rgr=regression( rgr_opt='knn',\n",
        "               no_of_selected_features=18)\n",
        "\n",
        "rgr.regression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LoKEepsDxpb"
      },
      "outputs": [],
      "source": [
        "##################################################################\n",
        "########## GRID SEARCH WITH SCALING ###########################\n",
        "##################################################################\n",
        "\n",
        "class regression():\n",
        "     def __init__(self,path='table.csv',rgr_opt='lr',no_of_selected_features=None):\n",
        "        self.path = path\n",
        "        self.rgr_opt=rgr_opt\n",
        "        self.no_of_selected_features=no_of_selected_features\n",
        "        if self.no_of_selected_features!=None:\n",
        "            self.no_of_selected_features=int(self.no_of_selected_features)\n",
        "\n",
        "     def regression_pipeline(self):\n",
        "\n",
        "    # Ridge Regression\n",
        "         if self.rgr_opt=='rg':\n",
        "            print('\\n\\t### Ridge Regression ### \\n')\n",
        "            rgr = Ridge(alpha=1.0,positive=True)\n",
        "            rgr_parameters = {\n",
        "            'rgr__solver':('auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs'),\n",
        "            }\n",
        "    # Linear Regression\n",
        "         elif self.rgr_opt=='lr':\n",
        "            print('\\n\\t### Linear Regression ### \\n')\n",
        "            rgr = LinearRegression()\n",
        "            rgr_parameters = {\n",
        "            'rgr__positive':(True,False),\n",
        "            }\n",
        "\n",
        "    # KNeighbors Regressor\n",
        "         elif self.rgr_opt=='knn':\n",
        "            print('\\n\\t### KNeighbors Regressor  ### \\n')\n",
        "            rgr = KNeighborsRegressor()\n",
        "            rgr_parameters = {\n",
        "            'rgr__n_neighbors':(3,5,8),\n",
        "            'rgr__weights':('uniform', 'distance')\n",
        "            }\n",
        "         else:\n",
        "            print('Select a valid classifier \\n')\n",
        "            sys.exit(0)\n",
        "         return rgr,rgr_parameters\n",
        "\n",
        "     def get_data(self):\n",
        "    # Load the file using Pandas\n",
        "        reader=pd.read_csv('table.csv')\n",
        "\n",
        "        data=reader.iloc[:, :-1]\n",
        "        labels=reader.iloc[:,-1]\n",
        "\n",
        "        # Training and Test Split\n",
        "        training_data, validation_data, training_cat, validation_cat = train_test_split(data, labels,\n",
        "                                               test_size=0.3, random_state=42)\n",
        "\n",
        "        scaler1 = StandardScaler()\n",
        "        scaler2 = RobustScaler()\n",
        "        scaler3 = MinMaxScaler()\n",
        "        # fit the scaler to the train set, it will learn the parameters\n",
        "        scaler2.fit(training_data)\n",
        "\n",
        "        # transform train and test sets\n",
        "        training_data_scaled = scaler2.transform(training_data)\n",
        "        validation_data_scaled = scaler2.transform(validation_data)\n",
        "\n",
        "        return training_cat, validation_cat,training_data_scaled,validation_data_scaled\n",
        "\n",
        "# Regression using the Gold Statndard after creating it from the raw text\n",
        "     def regression(self):\n",
        "   # Get the data\n",
        "        training_cat,validation_cat,training_data_scaled,validation_data_scaled=self.get_data()\n",
        "\n",
        "        rgr,rgr_parameters=self.regression_pipeline()\n",
        "        pipeline = Pipeline([('rgr', rgr),])\n",
        "        grid = GridSearchCV(pipeline,rgr_parameters,scoring='f1_macro',cv=10)\n",
        "        grid.fit(training_data_scaled,training_cat)\n",
        "        rgr= grid.best_estimator_\n",
        "        print('\\n\\n The best set of parameters of the pipiline are: ')\n",
        "        print(rgr)\n",
        "        joblib.dump(rgr, self.path+'training_model.joblib')\n",
        "        predicted=rgr.predict(validation_data_scaled)\n",
        "\n",
        "\n",
        "    # Regression report\n",
        "        mse=mean_squared_error(validation_cat,predicted,squared=True)\n",
        "        print ('\\n MSE:\\t'+str(mse))\n",
        "        rmse=mean_squared_error(validation_cat,predicted,squared=False)\n",
        "        print ('\\n RMSE:\\t'+str(rmse))\n",
        "        r2=r2_score(validation_cat,predicted,multioutput='variance_weighted')\n",
        "        print ('\\n R2-Score:\\t'+str(r2))\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "rgr=regression( rgr_opt='knn',\n",
        "               no_of_selected_features=18)\n",
        "\n",
        "rgr.regression()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqAEy-mLDxpb"
      },
      "source": [
        " DEFAULT PARAMTERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGey8uTGDxpb"
      },
      "outputs": [],
      "source": [
        "\n",
        "#################################################################\n",
        "#########  DEFAULT PARAMTERS WITHOUT SCLAING ####################\n",
        "#################################################################\n",
        "\n",
        "reader=pd.read_csv('table.csv')\n",
        "data=reader.iloc[:, :-1]\n",
        "labels=reader.iloc[:, -1]\n",
        "\n",
        "# Training and test split WITHOUT stratification\n",
        "training_data, validation_data, training_cat, validation_cat = train_test_split(data, labels,\n",
        "                                                test_size=0.30, random_state=42)\n",
        "\n",
        "print('\\n Training Data ')\n",
        "training_cat=[x for x in training_cat]\n",
        "\n",
        "print('\\n Validation Data ')\n",
        "validation_cat=[x for x in validation_cat]\n",
        "\n",
        "# Regression\n",
        "\n",
        "rgr1 = LinearRegression()\n",
        "rgr2 = Ridge(alpha=1.0,solver='lbfgs',positive=True)\n",
        "rgr4 = DecisionTreeRegressor(max_depth=12,ccp_alpha=0.02,random_state=10)\n",
        "rgr5= RandomForestRegressor(max_features=None)\n",
        "rgr6= SVR()\n",
        "rgr7= KNeighborsRegressor()\n",
        "\n",
        "rgr7.fit(training_data,training_cat)\n",
        "predicted=rgr7.predict(validation_data)\n",
        "\n",
        "# Regression report\n",
        "mse=mean_squared_error(validation_cat,predicted,squared=True)\n",
        "print ('\\n MSE:\\t'+str(mse))\n",
        "rmse=mean_squared_error(validation_cat,predicted,squared=False)\n",
        "print ('\\n RMSE:\\t'+str(rmse))\n",
        "r2=r2_score(validation_cat,predicted,multioutput='variance_weighted')\n",
        "print ('\\n R2-Score:\\t'+str(r2))\n",
        "\n",
        "\n",
        "###############################################\n",
        "####### WITH SCALING ##########################\n",
        "###############################################\n",
        "\n",
        "scaler2 = RobustScaler()\n",
        "\n",
        "# fit the scaler to the train set, it will learn the parameters\n",
        "scaler2.fit(training_data)\n",
        "\n",
        "# transform train and test sets\n",
        "training_data_scaled = scaler2.transform(training_data)\n",
        "validation_data_scaled = scaler2.transform(validation_data)\n",
        "\n",
        "rgr1 = LinearRegression()\n",
        "rgr2 = Ridge(alpha=1.0,solver='lbfgs',positive=True)\n",
        "rgr3 = Lasso(alpha=1.0)\n",
        "rgr4 = KNeighborsRegressor(n_neighbors=5)\n",
        "rgr5= RandomForestRegressor(max_features=None)\n",
        "\n",
        "rgr4.fit(training_data_scaled,training_cat)\n",
        "predicted=rgr4.predict(validation_data_scaled)\n",
        "\n",
        "# Regression report\n",
        "mse=mean_squared_error(validation_cat,predicted,squared=True)\n",
        "print ('\\n MSE:\\t'+str(mse))\n",
        "rmse=mean_squared_error(validation_cat,predicted,squared=False)\n",
        "print ('\\n RMSE:\\t'+str(rmse))\n",
        "r2=r2_score(validation_cat,predicted,multioutput='variance_weighted')\n",
        "print ('\\n R2-Score:\\t'+str(r2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1R7H_mq1Dxpc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}